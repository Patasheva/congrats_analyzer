{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1388BZBBpiCD",
        "outputId": "a973c973-f94b-4868-f55f-bdc4b0670ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting Flask-Cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: Flask-Cors\n",
            "Successfully installed Flask-Cors-5.0.1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qwen-vl-utils==0.0.8 (from qwen-vl-utils[decord]==0.0.8)\n",
            "  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting av (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8)\n",
            "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (11.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.32.3)\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils[decord]==0.0.8) (0.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Downloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av, qwen-vl-utils, bitsandbytes\n",
            "Successfully installed av-14.3.0 bitsandbytes-0.45.5 qwen-vl-utils-0.0.8\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=9fc1cf99fd84aa5b442b704aea18f48b9282147b4cb20a0b28fdb5b6cb7244da\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python-magic-bin (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for python-magic-bin\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,605 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,848 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,154 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [47.1 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,118 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,543 kB]\n",
            "Fetched 26.4 MB in 2s (11.1 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask Flask-Cors\n",
        "\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install -q git+https://github.com/huggingface/accelerate\n",
        "\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # spécifie l'index pour une installation potentiellement plus stable avec CUDA\n",
        "!pip install -q huggingface_hub sentencepiece protobuf\n",
        "\n",
        "!pip install -q moviepy opencv-python Pillow librosa soundfile decord\n",
        "\n",
        "!pip install qwen-vl-utils[decord]==0.0.8 einops timm bitsandbytes\n",
        "\n",
        "!pip install langdetect\n",
        "\n",
        "!pip install python-magic-bin # 'python-magic' nécessite souvent l'installation de 'python-magic-bin' sur Colab\n",
        "!pip install numpy\n",
        "\n",
        "!apt-get update && apt-get install -qq -y ffmpeg libmagic1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKZ5Vss2Gkn3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqlkfwaqG6YN",
        "outputId": "8103e381-5812-4be5-a0d1-e485b67d9a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRvYZgJGGWp1"
      },
      "outputs": [],
      "source": [
        " # Imports et Configuration Initiale\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import uuid # Peut être utile pour des noms uniques si besoin\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, AutoModelForCausalLM # Qwen 2.5 VL 3B\n",
        "from transformers import pipeline # Whisper\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from langdetect import detect, DetectorFactory\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "import cv2\n",
        "from google.colab import files # Pour l'upload\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n-bAfIiM5lT"
      },
      "outputs": [],
      "source": [
        "# Dossier temporaire dans Colab\n",
        "TEMP_FOLDER = \"/content/drive/MyDrive/Personas_project/Data/temp_files\"\n",
        "os.makedirs(TEMP_FOLDER, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h1WlxwniavM"
      },
      "source": [
        "Prompt.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F4cTUrGJHu9"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Act as an expert AI specialized in analyzing multimedia content to extract marketing insights for user persona creation.\n",
        "Your analysis must be objective and based solely on the provided information.\n",
        "You are analyzing content from a user-generated video uploaded to a congratulatory video creation platform.\n",
        "\n",
        "Analyze the provided image and the transcribed text.\n",
        "Your task is to extract **a single most likely value** for each field below, to help marketing teams build accurate personas.\n",
        "\n",
        "Output your results strictly in the following JSON structure (field names must match exactly):\n",
        "\n",
        "Rules:\n",
        "- For every field below, you must select **only one value** from the predefined list.\n",
        "  If you are **uncertain** or the content does not clearly support a choice, return `\"other\"` — do not guess or list multiple options.\n",
        "- Do **not list more than one category** for any field — only the most relevant one, or `\"other\"` if unclear.\n",
        "\n",
        "- For `number_of_people`, base your answer solely on the visual image. Count how many individual people are clearly visible.\n",
        "- If there is **more than one person**, return attributes (`gender`, `age`, `attire`) for each person separately, in a list.\n",
        "\n",
        "- For `age`, estimate in years and classify into one of the following ranges (return the range label as-is):\n",
        "\n",
        "    - \"0–2 years\"\n",
        "    - \"3–12 years\"\n",
        "    - \"13–17 years\"\n",
        "    - \"18–24 years\"\n",
        "    - \"25–34 years\"\n",
        "    - \"35–49 years\"\n",
        "    - \"50–64 years\"\n",
        "    - \">=65 years\"\n",
        "\n",
        "- Gender and attire must also be based on visible cues only. If uncertain, return `\"other\"`.\n",
        "\n",
        "JSON format:\n",
        "\n",
        "QA = {\n",
        "  \"number_of_people\": \"1 | 2 | 3 | 4 | 5 | >5\",\n",
        "  \"people\": [\n",
        "    {\n",
        "      \"gender\": \"female | male | other\",\n",
        "      \"age\": \"0–2 years | 3–12 years | 13–17 years | 18–24 years | 25–34 years | 35–49 years | 50–64 years | >=65 years | other\",\n",
        "      \"attire\": \"casual | formal | business | sport | party | uniform | other\"\n",
        "    }\n",
        "  ],\n",
        "  \"recording_location\": \"home | living room | kitchen | bedroom | garden | outdoor | party | office | meeting room | open space | stage | car | other\",\n",
        "  \"motivation\": \"personal | professional | other\",\n",
        "  \"occasion\": \"birthday | wedding | engagement | birth | baptism | love message | mother's day | father's day | farewell | get well | fun video | graduation | work anniversary | promotion | new job | success | team celebration | product presentation | company presentation | conference | retirement | new year | christmas | valentine's day | halloween | easter | international women's day | other\",\n",
        "  \"viral_mood\": \"happy | excited | fun | proud | grateful | emotional | festive | nostalgic | loving | motivated | inspirational | formal | other\",\n",
        "  \"relationship\": \"mother | father | sister | brother | spouse | partner | husband | wife | child | relative | boyfriend | girlfriend | fiancé | friend | best friend | colleague | boss | employee | teacher | student | client | lead | other\"\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2VlwV7FA1bB"
      },
      "source": [
        "Utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkWdXhT6J5V4",
        "outputId": "3c81e10d-9e88-48c8-ee6a-7c9712f5f83a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement du modèle de transcription: openai/whisper-base...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n",
            "Device set to use cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle de transcription chargé.\n"
          ]
        }
      ],
      "source": [
        "# --- Initialisation Speech-to-Text (Whisper)---\n",
        "\n",
        "WHISPER_MODEL = \"openai/whisper-base\"\n",
        "print(f\"Chargement du modèle de transcription: {WHISPER_MODEL}...\")\n",
        "try:\n",
        "    transcription_pipeline = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=WHISPER_MODEL,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"Modèle de transcription chargé.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement du modèle Whisper: {e}\")\n",
        "    transcription_pipeline = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "0afa63c66a9d4472b576ffc1865bb7a9",
            "51ed835a0f6a4d758391d3aaa8f6bb44",
            "fa171f039c78416bb21f529e7429bdd8",
            "6039ad448ac94a6cb55e32e5e07af9bf",
            "d62ead27549245e3938b91bfb81bdb5a",
            "876c24061588466ca2c5f342dd61338e",
            "c929013f32374143b23172caa6052053",
            "2f8cf2942087412d88112b855d0158da",
            "6a6c2af448334bd391c651169c13575d",
            "c556470cfd5e415aaa12694d873b1271",
            "a985b51ec1614d399b34e78629b75c5d"
          ]
        },
        "id": "QclF1xMGc8-P",
        "outputId": "ea5e6674-15db-4aa3-ca4f-51b3911831d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0afa63c66a9d4472b576ffc1865bb7a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Initialisation VLM (QWEN2.5 VL 3B)---\n",
        "QWEN_MODEL = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "qwen_vl_processor = AutoProcessor.from_pretrained(QWEN_MODEL, trust_remote_code=True)\n",
        "qwen_vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    QWEN_MODEL,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6PEcXoNS8Kr"
      },
      "outputs": [],
      "source": [
        "# --- Fonctions Utilitaires Video Preprosessing ---\n",
        "\n",
        "def generate_unique_filename(directory, extension):\n",
        "    \"\"\"Génère un nom de fichier unique dans un dossier.\"\"\"\n",
        "    while True:\n",
        "        filename = str(uuid.uuid4()) + extension\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if not os.path.exists(filepath):\n",
        "            return filepath\n",
        "\n",
        "def extract_middle_frame(video_path, output_dir=TEMP_FOLDER):\n",
        "    \"\"\"Extrait l'image du milieu et la retourne en tant qu'objet PIL Image.\"\"\"\n",
        "    print(f\"Extraction de l'image du milieu de : {video_path}\")\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(\"Erreur: Impossible d'ouvrir la vidéo.\")\n",
        "            return None\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if total_frames == 0:\n",
        "             print(\"Erreur: La vidéo ne contient aucune image.\")\n",
        "             cap.release()\n",
        "             return None\n",
        "        middle_frame_index = total_frames // 2\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_index)\n",
        "        ret, frame = cap.read()\n",
        "        cap.release()\n",
        "        if ret:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            image = Image.fromarray(frame_rgb)\n",
        "            print(\"Image du milieu extraite avec succès.\")\n",
        "            return image\n",
        "        else:\n",
        "            print(\"Erreur: Impossible de lire l'image du milieu.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'extraction de l'image : {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_audio(video_path, output_dir=TEMP_FOLDER):\n",
        "    \"\"\"Extrait l'audio et retourne le chemin du fichier audio temporaire.\"\"\"\n",
        "    print(f\"Extraction de l'audio de : {video_path}\")\n",
        "    audio_path = generate_unique_filename(output_dir, \".wav\")\n",
        "    try:\n",
        "        with VideoFileClip(video_path) as video_clip:\n",
        "            audio_clip = video_clip.audio\n",
        "            if audio_clip:\n",
        "                audio_clip.write_audiofile(audio_path, codec='pcm_s16le', logger=None) # logger=None pour moins de verbosité\n",
        "                audio_clip.close()\n",
        "                print(f\"Audio extrait et sauvegardé dans : {audio_path}\")\n",
        "                return audio_path\n",
        "            else:\n",
        "                print(\"Aucune piste audio trouvée dans la vidéo.\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'extraction de l'audio : {e}\")\n",
        "        if os.path.exists(audio_path): os.remove(audio_path)\n",
        "        return None\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Transcrire le fichier audio.\"\"\"\n",
        "    if not transcription_pipeline:\n",
        "        print(\"Modèle de transcription non chargé. Transcription ignorée.\")\n",
        "        return \"\"\n",
        "    if not audio_path or not os.path.exists(audio_path):\n",
        "        print(\"Chemin audio invalide ou fichier manquant. Transcription ignorée.\")\n",
        "        return \"\"\n",
        "    print(f\"Transcription de l'audio : {audio_path}\")\n",
        "    try:\n",
        "        # Forcer la transcription sur le device choisi (CPU ou GPU)\n",
        "        result = transcription_pipeline(audio_path)\n",
        "        transcription = result[\"text\"]\n",
        "        print(\"Transcription terminée.\")\n",
        "        return transcription\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la transcription audio : {e}\")\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsLzm9ktw3pz",
        "outputId": "6dc38088-7409-4a56-a6c0-a0b53db4ecca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fonctions utilitaires définies.\n"
          ]
        }
      ],
      "source": [
        "# --- Fonctions Utilitaires Analyze Content ---\n",
        "temp_image_path = None\n",
        "def analyze_content(middle_image_obj, transcribed_text, prompt, max_new_tokens=2048):\n",
        "    \"\"\"Analyse l'image (PIL) et le texte avec Qwen-VL.\"\"\"\n",
        "    if middle_image_obj is None:\n",
        "        print(\"Analyse annulée : image manquante.\")\n",
        "        return json.dumps({\"error\": \"Image non fournie ou invalide.\"})\n",
        "\n",
        "    print(\"Début de l'analyse multimodale...\")\n",
        "\n",
        "    if qwen_vl_model and qwen_vl_processor:\n",
        "        # --- Exécution ---\n",
        "        try:\n",
        "            print(\"Génération de la réponse par Qwen-VL...\")\n",
        "            # Construire la query pour Qwen2.5-VL 3B\n",
        "            temp_image_path = generate_unique_filename(TEMP_FOLDER, \".jpg\")\n",
        "            middle_image_obj.save(temp_image_path)\n",
        "\n",
        "\n",
        "            messages = [\n",
        "                 {\"role\": \"user\", \"content\": [\n",
        "                         {\"type\": \"image\",\"image\": temp_image_path},\n",
        "                         {\"type\": \"text\", \"text\": f\"{prompt}\\n\\nTranscription Audio: \\\"{transcribed_text}\\\"\"},\n",
        "                     ],\n",
        "                 }\n",
        "            ]\n",
        "\n",
        "            # Preparation for inference\n",
        "            text = qwen_vl_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "            image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "            inputs = qwen_vl_processor(\n",
        "                text=[text],\n",
        "                images=image_inputs,\n",
        "                videos=video_inputs,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "            inputs = inputs.to(qwen_vl_model.device)\n",
        "\n",
        "            # Inference: Generation of the output\n",
        "            output_ids = qwen_vl_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "            generated_ids = [out_ids[len(input_ids) :] for input_ids, out_ids in zip(inputs.input_ids, output_ids)]\n",
        "            output_text_list= qwen_vl_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "            print(\"Réponse Qwen-VL reçue.\")\n",
        "\n",
        "            if output_text_list:\n",
        "               analysis_result = output_text_list[0] # Utiliser directement la première chaîne\n",
        "            else:\n",
        "                analysis_result = json.dumps({\"error\": \"Réponse du modèle vide.\"})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de l'inférence avec Qwen-VL local : {e}\")\n",
        "            analysis_result = json.dumps({\"error\": f\"Erreur d'inférence Qwen-VL: {e}\"})\n",
        "        finally:\n",
        "            if os.path.exists(temp_image_path): os.remove(temp_image_path)\n",
        "            return analysis_result\n",
        "    else:\n",
        "        # --- SIMULATION ---\n",
        "        print(\"Simulation de l'analyse Qwen-VL.\")\n",
        "        simulated_output = {\n",
        "            \"motivation\": \"personal\", \"occasion\": \"birthday\", \"recording_location\": \"living room\",\n",
        "            \"number_of_people\": \"1\",\n",
        "            \"people\": [{\"gender\": \"male\", \"age_category\": \"young_adult\", \"attire\": \"casual\"}],\n",
        "            \"viral_mood\": \"happy\", \"relationship\": \"friend\"\n",
        "        }\n",
        "        return json.dumps(simulated_output, indent=2)\n",
        "\n",
        "def cleanup_files(*args):\n",
        "    \"\"\"Supprime les fichiers temporaires spécifiés.\"\"\"\n",
        "    for file_path in args:\n",
        "        if file_path and os.path.exists(file_path):\n",
        "            try:\n",
        "                os.remove(file_path)\n",
        "                print(f\"Fichier temporaire supprimé : {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lors de la suppression du fichier {file_path}: {e}\")\n",
        "\n",
        "print(\"Fonctions utilitaires définies.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WeNT8iXiKgw"
      },
      "source": [
        "Upload test video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpc2gLG7iXNS"
      },
      "outputs": [],
      "source": [
        "DRIVE_VIDEOS_PATH = \"/content/drive/MyDrive/Personas_project/Data/videos/Vidday_videos\"\n",
        "\n",
        "def choisir_video_drive():\n",
        "    \"\"\"Affiche les vidéos disponibles sur Google Drive et permet à l'utilisateur d'en choisir une.\"\"\"\n",
        "    try:\n",
        "        video_files = [f for f in os.listdir(DRIVE_VIDEOS_PATH) if f.endswith(('.mp4', '.mov', '.avi', '.mkv'))]\n",
        "        if not video_files:\n",
        "            print(f\"Aucun fichier vidéo trouvé dans le dossier : {DRIVE_VIDEOS_PATH}\")\n",
        "            return None\n",
        "\n",
        "        print(\"Vidéos disponibles :\")\n",
        "        for i, filename in enumerate(video_files):\n",
        "            print(f\"{i + 1}. {filename}\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                choix = input(\"Veuillez entrer le numéro de la vidéo à utiliser : \")\n",
        "                index = int(choix) - 1\n",
        "                if 0 <= index < len(video_files):\n",
        "                    video_filename = video_files[index]\n",
        "                    video_path = os.path.join(DRIVE_VIDEOS_PATH, video_filename)\n",
        "                    print(f\"Vidéo sélectionnée : {video_path}\")\n",
        "                    return video_path\n",
        "                else:\n",
        "                    print(\"Choix invalide. Veuillez entrer un numéro de la liste.\")\n",
        "            except ValueError:\n",
        "                print(\"Entrée invalide. Veuillez entrer un numéro.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Le dossier spécifié est introuvable : {DRIVE_VIDEOS_PATH}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Une erreur est survenue lors de la lecture du dossier : {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUONV2fZChFY",
        "outputId": "3537546d-3529-4180-e7f7-d6ca7fa7f268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vidéos disponibles :\n",
            "1. Birthday_twin.mp4\n",
            "2. Fathersday_boy.mp4\n",
            "3. Graduation_1.mp4\n",
            "4. Funny_Birthday_Memories.mp4\n",
            "5. Graduation_family.mp4\n",
            "6. Teacher_appreciation.mp4\n",
            "7. Wedding.mp4\n",
            "8. Wedding_wishes.mp4\n",
            "9. Birthday_friends.mp4\n",
            "Veuillez entrer le numéro de la vidéo à utiliser : 1\n",
            "Vidéo sélectionnée : /content/drive/MyDrive/Personas_project/Data/videos/Vidday_videos/Birthday_twin.mp4\n"
          ]
        }
      ],
      "source": [
        "video_path = choisir_video_drive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbT4P-0yiZZ4"
      },
      "source": [
        "App.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEj4gRBPAIw4",
        "outputId": "ecd371d7-14a7-4d91-c37b-4097d64fe235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction de l'image du milieu de : /content/drive/MyDrive/Personas_project/Data/videos/Vidday_videos/Birthday_twin.mp4\n",
            "Image du milieu extraite avec succès.\n",
            "Extraction de l'audio de : /content/drive/MyDrive/Personas_project/Data/videos/Vidday_videos/Birthday_twin.mp4\n",
            "Audio extrait et sauvegardé dans : /content/drive/MyDrive/Personas_project/Data/temp_files/71c63069-4918-4720-a1fe-c0a25847ad9f.wav\n",
            "Transcription de l'audio : /content/drive/MyDrive/Personas_project/Data/temp_files/71c63069-4918-4720-a1fe-c0a25847ad9f.wav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription terminée.\n",
            "--------------------\n",
            "Transcription:  From your very old personal birthday to in, thank you for being an amazing leader and a wonderful human being. We're so happy to have you on this planet, happy birthday and enjoy your special day.\n",
            "--------------------\n",
            "Début de l'analyse multimodale...\n",
            "Génération de la réponse par Qwen-VL...\n",
            "Réponse Qwen-VL reçue.\n",
            "Nettoyage des fichiers temporaires...\n",
            "Fichier temporaire supprimé : /content/drive/MyDrive/Personas_project/Data/temp_files/71c63069-4918-4720-a1fe-c0a25847ad9f.wav\n",
            "Nettoyage terminé.\n",
            "\n",
            "============================== RÉSULTAT DE L'ANALYSE ==============================\n",
            "```json\n",
            "{\n",
            "  \"number_of_people\": \"1\",\n",
            "  \"people\": [\n",
            "    {\n",
            "      \"gender\": \"female\",\n",
            "      \"age\": \"25–34 years\",\n",
            "      \"attire\": \"casual\"\n",
            "    }\n",
            "  ],\n",
            "  \"recording_location\": \"outdoor\",\n",
            "  \"motivation\": \"personal\",\n",
            "  \"occasion\": \"birthday\",\n",
            "  \"viral_mood\": \"happy\",\n",
            "  \"relationship\": \"friend\"\n",
            "}\n",
            "```\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Exécuter le processus complet ---\n",
        "analysis_result_json = None\n",
        "middle_image_obj = None\n",
        "audio_file_path = None\n",
        "transcribed_text = \"\"\n",
        "\n",
        "if video_path and os.path.exists(video_path):\n",
        "    try:\n",
        "        # 1. Extraire l'image du milieu\n",
        "        middle_image_obj = extract_middle_frame(video_path)\n",
        "\n",
        "        # 2. Extraire l'audio\n",
        "        audio_file_path = extract_audio(video_path)\n",
        "\n",
        "        # 3. Transcrire l'audio\n",
        "        if audio_file_path:\n",
        "            transcribed_text = transcribe_audio(audio_file_path)\n",
        "            print(\"-\" * 20)\n",
        "            print(f\"Transcription: {transcribed_text}\")\n",
        "            print(\"-\" * 20)\n",
        "        else:\n",
        "            print(\"Pas d'audio à transcrire.\")\n",
        "\n",
        "        # 4. Analyser image et texte\n",
        "        if middle_image_obj:\n",
        "            analysis_result_json = analyze_content(middle_image_obj, transcribed_text, prompt)\n",
        "        else:\n",
        "             analysis_result_json = json.dumps({\"error\": \"Impossible de procéder à l'analyse sans image.\"}, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERREUR MAJEURE pendant le pipeline d'analyse : {e}\")\n",
        "        analysis_result_json = json.dumps({\"error\": f\"Erreur pipeline: {e}\"}, indent=2)\n",
        "    finally:\n",
        "        # 5. Nettoyer les fichiers temporaires (vidéo originale uploadée et audio extrait)\n",
        "        print(\"Nettoyage des fichiers temporaires...\")\n",
        "        cleanup_files(audio_file_path, temp_image_path)\n",
        "        print(\"Nettoyage terminé.\")\n",
        "\n",
        "else:\n",
        "    print(\"Chemin vidéo invalide. Impossible de lancer l'analyse.\")\n",
        "    analysis_result_json = json.dumps({\"error\": \"Fichier vidéo non trouvé ou non uploadé.\"}, indent=2)\n",
        "\n",
        "\n",
        "# --- Afficher le Résultat Final ---\n",
        "print(\"\\n\" + \"=\"*30 + \" RÉSULTAT DE L'ANALYSE \" + \"=\"*30)\n",
        "if analysis_result_json:\n",
        "    # Imprimer le JSON formaté\n",
        "    print(analysis_result_json)\n",
        "    # Vous pouvez aussi le charger en dictionnaire Python si besoin\n",
        "    # try:\n",
        "    #    result_dict = json.loads(analysis_result_json)\n",
        "    #    print(\"\\n(Résultat en tant que dictionnaire Python):\")\n",
        "    #    print(result_dict)\n",
        "    # except json.JSONDecodeError:\n",
        "    #    print(\"\\nImpossible de parser le résultat en dictionnaire Python.\")\n",
        "\n",
        "else:\n",
        "    print(\"Aucun résultat d'analyse généré.\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0afa63c66a9d4472b576ffc1865bb7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51ed835a0f6a4d758391d3aaa8f6bb44",
              "IPY_MODEL_fa171f039c78416bb21f529e7429bdd8",
              "IPY_MODEL_6039ad448ac94a6cb55e32e5e07af9bf"
            ],
            "layout": "IPY_MODEL_d62ead27549245e3938b91bfb81bdb5a"
          }
        },
        "2f8cf2942087412d88112b855d0158da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ed835a0f6a4d758391d3aaa8f6bb44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_876c24061588466ca2c5f342dd61338e",
            "placeholder": "​",
            "style": "IPY_MODEL_c929013f32374143b23172caa6052053",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6039ad448ac94a6cb55e32e5e07af9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c556470cfd5e415aaa12694d873b1271",
            "placeholder": "​",
            "style": "IPY_MODEL_a985b51ec1614d399b34e78629b75c5d",
            "value": " 2/2 [00:31&lt;00:00, 15.77s/it]"
          }
        },
        "6a6c2af448334bd391c651169c13575d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "876c24061588466ca2c5f342dd61338e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a985b51ec1614d399b34e78629b75c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c556470cfd5e415aaa12694d873b1271": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c929013f32374143b23172caa6052053": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d62ead27549245e3938b91bfb81bdb5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa171f039c78416bb21f529e7429bdd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f8cf2942087412d88112b855d0158da",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a6c2af448334bd391c651169c13575d",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
